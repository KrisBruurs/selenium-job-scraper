{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817dc43e-812a-4d25-ba4d-8374979c8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebDriver initialized.\n",
      "\n",
      "--- Phase 1: Collecting Job Links ---\n",
      "Scraping page 1: https://weworkremotely.com/top-trending-remote-jobs?page=1\n",
      "Cookies accepted (or attempted).\n",
      "Found 51 potential job listings on page 1\n",
      "Scraping page 2: https://weworkremotely.com/top-trending-remote-jobs?page=2\n",
      "Found 51 potential job listings on page 2\n",
      "Scraping page 3: https://weworkremotely.com/top-trending-remote-jobs?page=3\n",
      "Found 51 potential job listings on page 3\n",
      "\n",
      "Total unique job links collected: 150\n",
      "Total unique job links after deduplication: 150\n",
      "\n",
      "--- Phase 2: Scraping Detailed Job Data ---\n",
      "Processing job 1/150: https://weworkremotely.com/remote-jobs/bbe-marketing-inc-account-manager-8\n",
      "Processing job 2/150: https://weworkremotely.com/remote-jobs/speechify-inc-tech-lead-web-core-product-chrome-extension\n",
      "Processing job 3/150: https://weworkremotely.com/remote-jobs/files-com-ui-ux-designer-2\n",
      "Processing job 4/150: https://weworkremotely.com/remote-jobs/evaboot-customer-support-build-the-ai-that-will-steal-your-job\n",
      "Processing job 5/150: https://weworkremotely.com/remote-jobs/speechify-inc-software-engineer-platform\n",
      "Processing job 6/150: https://weworkremotely.com/remote-jobs/bbe-marketing-inc-graphic-designer-social-media-manager\n",
      "Processing job 7/150: https://weworkremotely.com/remote-jobs/clipboard-health-client-support-specialist-healthcare-facilities-b2b-1\n",
      "Processing job 8/150: https://weworkremotely.com/remote-jobs/sweat-pants-agency-media-buyer-1\n",
      "Processing job 9/150: https://weworkremotely.com/remote-jobs/forager-growth-marketing-manager\n",
      "Processing job 10/150: https://weworkremotely.com/remote-jobs/packtrip-land-staff-accountant\n",
      "Processing job 11/150: https://weworkremotely.com/remote-jobs/a-insurance-designers-senior-digital-marketing-specialist\n",
      "Processing job 12/150: https://weworkremotely.com/remote-jobs/the-lawgical-firm-p-a-virtual-assistant-bilingual-with-vietnamese-and-english-required\n",
      "Processing job 13/150: https://weworkremotely.com/remote-jobs/mailerlite-web-designer-2\n",
      "Processing job 14/150: https://weworkremotely.com/remote-jobs/volcano-builders-remote-administrative-operations-assistant\n",
      "Processing job 15/150: https://weworkremotely.com/remote-jobs/smib-business-development-executive\n",
      "Processing job 16/150: https://weworkremotely.com/remote-jobs/ima-financial-group-account-executive-underwriter\n",
      "Processing job 17/150: https://weworkremotely.com/remote-jobs/brook-hiddink-highticket-io-a-i-video-editor-direct-response-veo3\n",
      "Processing job 18/150: https://weworkremotely.com/remote-jobs/the-humble-hackers-generative-ai-trends-analyst-part-time-remote\n",
      "Processing job 19/150: https://weworkremotely.com/remote-jobs/telus-digital-us-internet-rater-english-language\n",
      "Processing job 20/150: https://weworkremotely.com/remote-jobs/canara-bank-securities-limited-freelance-facebook-google-ads-expert-india-remote-10k-15k-per\n",
      "Processing job 21/150: https://weworkremotely.com/remote-jobs/accesa-senior-ux-ui-designer-with-german\n",
      "Processing job 22/150: https://weworkremotely.com/remote-jobs/pr-volt-outbound-sales-specialist\n",
      "Processing job 23/150: https://weworkremotely.com/remote-jobs/melapress-wordpress-plugins-themes-developer\n",
      "Processing job 24/150: https://weworkremotely.com/remote-jobs/spiralyze-ui-ux-designer\n",
      "Processing job 25/150: https://weworkremotely.com/remote-jobs/proudfoot-proudfoot-presenting-analyst-united-kingdom-europe-the-americas\n",
      "Processing job 26/150: https://weworkremotely.com/remote-jobs/paymentology-implementation-project-manager-2\n",
      "Processing job 27/150: https://weworkremotely.com/remote-jobs/sana-vita-operations-llc-project-coordinator\n",
      "Processing job 28/150: https://weworkremotely.com/remote-jobs/vidalytics-europe-only-growth-product-manager-at-video-martech-saas\n",
      "Processing job 29/150: https://weworkremotely.com/remote-jobs/scholarshipowl-affiliate-manager\n",
      "Processing job 30/150: https://weworkremotely.com/remote-jobs/mint-lily-ecommerce-operations-manager\n",
      "Processing job 31/150: https://weworkremotely.com/remote-jobs/howie-ai-remote-scheduling-calendar-assistant-1\n",
      "Processing job 32/150: https://weworkremotely.com/remote-jobs/lumenalta-javascript-fullstack-engineer-senior\n",
      "Processing job 33/150: https://weworkremotely.com/remote-jobs/apicworld-finance-accounting-specialist-lithuania-estonia\n",
      "Processing job 34/150: https://weworkremotely.com/remote-jobs/creative-force-product-specialist-jedi-german-speaking\n",
      "Processing job 35/150: https://weworkremotely.com/remote-jobs/melapress-content-led-marketer-b2b-startup-mentality\n",
      "Processing job 36/150: https://weworkremotely.com/remote-jobs/hey-contact-heroes-gmbh-eu-freelancer-m-w-d-im-inbound-kundenservice-home-office-1\n",
      "Processing job 37/150: https://weworkremotely.com/remote-jobs/hb-travels-remote-travel-booking-specialists\n",
      "Processing job 38/150: https://weworkremotely.com/remote-jobs/contra-content-creators-for-viral-video-project\n",
      "Processing job 39/150: https://weworkremotely.com/remote-jobs/appmagic-senior-sales-manager\n",
      "Processing job 40/150: https://weworkremotely.com/remote-jobs/proxify-ab-senior-django-developer-1\n",
      "Processing job 41/150: https://weworkremotely.com/remote-jobs/waifu-masters-head-of-social-social-media-manager\n",
      "Processing job 42/150: https://weworkremotely.com/remote-jobs/subscript-software-engineer-frontend-backend-or-full-stack-2\n",
      "Processing job 43/150: https://weworkremotely.com/remote-jobs/crisp-ui-ux-designer\n",
      "Processing job 44/150: https://weworkremotely.com/remote-jobs/intellisync-srl-senior-software-engineer\n",
      "Processing job 45/150: https://weworkremotely.com/remote-jobs/flag-theory-ifrs-accountant-with-multiple-jurisdiction-experience\n",
      "Processing job 46/150: https://weworkremotely.com/remote-jobs/credit-clerk-inside-sales-representative-u-s-market\n",
      "Processing job 47/150: https://weworkremotely.com/remote-jobs/mailerlite-product-designer-3\n",
      "Processing job 48/150: https://weworkremotely.com/remote-jobs/escorial-future-digital-asset-operations\n",
      "Processing job 49/150: https://weworkremotely.com/remote-jobs/dataannotation-tech-ft-pt-remote-ai-prompt-engineering-evaluation-will-train\n",
      "Processing job 50/150: https://weworkremotely.com/remote-jobs/ae-virtual-class-s-a-languages-teachers\n",
      "Processing job 51/150: https://weworkremotely.com/remote-jobs/canva-staff-backend-engineer-java-admin-experience-open-to-remote-across-anz\n",
      "Processing job 52/150: https://weworkremotely.com/remote-jobs/zapiet-experienced-laravel-developer-apac-region\n",
      "Processing job 53/150: https://weworkremotely.com/remote-jobs/hummingbird-revops-manager\n",
      "Processing job 54/150: https://weworkremotely.com/remote-jobs/amazowl-amazon-ecommerce-account-manager-seller-vendor\n",
      "Processing job 55/150: https://weworkremotely.com/remote-jobs/baymard-institute-financial-customer-support-accounts-receivables-employee\n",
      "Processing job 56/150: https://weworkremotely.com/remote-jobs/tripleten-creative-producer-remote-ft-contract\n",
      "Error scraping details for link https://weworkremotely.com/remote-jobs/tripleten-creative-producer-remote-ft-contract: HTTPConnectionPool(host='localhost', port=51555): Read timed out. (read timeout=120)\n",
      "Processing job 57/150: https://weworkremotely.com/remote-jobs/sa-global-dynamics-365-customer-engagement-ce-technical-architect\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import re\n",
    "import json # If you decide to output JSON first\n",
    "\n",
    "def accept_cookies(driver_instance):\n",
    "    \"\"\"Attempts to accept cookie consent popups.\"\"\"\n",
    "    try:\n",
    "        # Prioritize more common/broad selectors\n",
    "        # These are examples, you might need to combine or try more specific ones\n",
    "        WebDriverWait(driver_instance, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.fc-button, button.adroll_button_text, #adroll_consent_allow_all, .fc-cta-consent'))).click()\n",
    "        # Give it a moment for popups to disappear\n",
    "        time.sleep(1)\n",
    "        print(\"Cookies accepted (or attempted).\")\n",
    "    except Exception:\n",
    "        print(\"Cookie buttons not found or already accepted/handled.\")\n",
    "\n",
    "def scrape_weworkremotely():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\") # Only if you need the full window, might be slower for headless\n",
    "\n",
    "    driver = uc.Chrome(options=options)\n",
    "    print(\"WebDriver initialized.\")\n",
    "\n",
    "    base_url = \"https://weworkremotely.com/top-trending-remote-jobs?page=\"\n",
    "    urls = [f\"{base_url}{i}\" for i in range(1, 4)] # scrape pages 1 to 3\n",
    "\n",
    "    all_job_links = []\n",
    "    job_data = [] # Data container for detailed job info\n",
    "\n",
    "    try: # Main try block for overall process\n",
    "        # === Phase 1: Collect Job Links ===\n",
    "        print(\"\\n--- Phase 1: Collecting Job Links ---\")\n",
    "        for index, url in enumerate(urls):\n",
    "            driver.get(url)\n",
    "            print(f\"Scraping page {index + 1}: {url}\")\n",
    "\n",
    "            # Attempt to accept cookies on the first page load or any page load\n",
    "            if index == 0: # Or just call it on the first successful page load\n",
    "                accept_cookies(driver)\n",
    "\n",
    "            # Scroll to bottom to load all info\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "\n",
    "            # Scrape job links\n",
    "            job_links_elements = driver.find_elements(By.CSS_SELECTOR, \"section.jobs li > a\")\n",
    "            print(f\"Found {len(job_links_elements)} potential job listings on page {index + 1}\")\n",
    "\n",
    "            page_links = [\n",
    "                link.get_attribute(\"href\")\n",
    "                for link in job_links_elements\n",
    "                if link.get_attribute(\"href\") and link.get_attribute(\"href\") != \"https://weworkremotely.com/\"\n",
    "            ]\n",
    "            all_job_links.extend(page_links)\n",
    "\n",
    "            time.sleep(random.uniform(2, 5)) # Random delay between pages\n",
    "\n",
    "        print(f\"\\nTotal unique job links collected: {len(all_job_links)}\")\n",
    "        # You might want to remove duplicates here too\n",
    "        all_job_links = list(set(all_job_links))\n",
    "        print(f\"Total unique job links after deduplication: {len(all_job_links)}\")\n",
    "\n",
    "\n",
    "        # === Phase 2: Scrape Detailed Job Data ===\n",
    "        print(\"\\n--- Phase 2: Scraping Detailed Job Data ---\")\n",
    "        for i, link in enumerate(all_job_links):\n",
    "            print(f\"Processing job {i+1}/{len(all_job_links)}: {link}\")\n",
    "            try:\n",
    "                driver.get(link)\n",
    "\n",
    "                # Wait for main elements to be present before scraping\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'lis-container__header__hero__company-info__title')))\n",
    "\n",
    "                job_title = driver.find_element(By.CLASS_NAME, 'lis-container__header__hero__company-info__title').text\n",
    "                company_block = driver.find_element(By.CLASS_NAME, 'lis-container__job__sidebar__companyDetails__info')\n",
    "                company_title = company_block.find_element(By.CLASS_NAME, 'lis-container__job__sidebar__companyDetails__info__title').text\n",
    "\n",
    "                try:\n",
    "                    jobs_posted_text = WebDriverWait(company_block, 10).until( # Reduced wait time\n",
    "                        EC.visibility_of_element_located((By.CLASS_NAME, 'lis-container__job__sidebar__companyDetails__info__jobs-posted'))\n",
    "                    ).text.strip()\n",
    "                except:\n",
    "                    jobs_posted_text = \"NA\"\n",
    "                    print(f\"  Warning: 'Jobs Posted' not found for {link}\")\n",
    "\n",
    "                job_block = driver.find_element(By.CLASS_NAME, 'lis-container__job__sidebar__job-about')\n",
    "                category_items = job_block.find_elements(By.CLASS_NAME, 'lis-container__job__sidebar__job-about__list__item')\n",
    "\n",
    "                job_details = {}\n",
    "                for item in category_items:\n",
    "                    lines = item.text.strip().split(\"\\n\")\n",
    "                    if len(lines) >= 2:\n",
    "                        label = lines[0].strip()\n",
    "                        value = \"\\n\".join(lines[1:]).strip()\n",
    "                        job_details[label] = value\n",
    "\n",
    "                region_raw = job_details.get(\"Region\", \"NA\")\n",
    "                # Simplified region extraction\n",
    "                region_extracted = \"NA\"\n",
    "                if \" Only\" in region_raw:\n",
    "                    region_extracted = region_raw.split(\" Only\")[0].strip() + \" Only\"\n",
    "                elif region_raw != \"NA\": # If it's not \"NA\" but also not \"X Only\"\n",
    "                    region_extracted = region_raw # Keep the raw value if it's not an \"Only\" pattern\n",
    "\n",
    "                job_entry = {\n",
    "                    \"Ranking\": i + 1, # This ranking will be based on scrape order, not website ranking\n",
    "                    \"Job Title\": job_title,\n",
    "                    \"Company\": company_title,\n",
    "                    \"Jobs Posted\": jobs_posted_text.replace(\"Jobs posted: \", \"\"),\n",
    "                    \"Job Type\": job_details.get(\"Job type\", \"NA\"),\n",
    "                    \"Category\": job_details.get(\"Category\", \"NA\"),\n",
    "                    \"Salary\": job_details.get(\"Salary\", \"NA\"),\n",
    "                    \"Region\": region_extracted,\n",
    "                    \"Link\": link\n",
    "                }\n",
    "\n",
    "                job_data.append(job_entry)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping details for link {link}: {e}\")\n",
    "                # Optional: log failed links to a separate list\n",
    "                # failed_links.append(link)\n",
    "\n",
    "            time.sleep(random.uniform(1, 3)) # Variable delay between detail requests\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error during scraping process: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nWebDriver closed.\")\n",
    "\n",
    "    # Write to CSV\n",
    "    if job_data: # Only write if there's data\n",
    "        output_file_name = \"top_trending_jobs.csv\"\n",
    "        with open(output_file_name, \"w\", newline='', encoding='utf-8') as f:\n",
    "            fieldnames = list(job_data[0].keys()) # Get headers from first entry\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(job_data)\n",
    "        print(f\"Scraped {len(job_data)} jobs and saved to '{output_file_name}'\")\n",
    "    else:\n",
    "        print(\"No job data scraped. CSV file not created.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_weworkremotely()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
